---
title: "W05_3_Solution"
author: "Adela Sobotkova"
date: "12/03/2021"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# Task 3: How to best classify the IKONOS imagery in Kazanlak?

Being able to classify imagery and observe change over time is helpful when development management plans for environment and cultural heritage alike. Besides the golden treasures of past kings, Kazanlak is also the world-famous producer of the rose attar. Most of it is sold to Japan for cosmetics today, while the industry started under the Ottomans, who kept their harems happy with the rose water and oil. Can you tell how much ground is covered with the rose fields today? (Hint: roses were marked under perennial category during field survey, and teams generally avoided them, so many will be outside the survey area due to poor ground visibility). Practice classification of categorical rasters in R by conducting and comparing unsupervised (kmeans) and supervised (Random Forest) classification. For the Random FOrest, follow [this manual](https://geoscripting-wur.github.io/AdvancedRasterAnalysis/)

* Choose a subset of the area, or downsample your raster
* Run kmeans AND random forests classification on IKONOS satellite imagery from Bulgaria
* Evaluate and compare both models
* In order to train your Random Forest classification, you may use the KAZ_units.shp and KAZ_teamsdata.csv, which contain information on landuse (annual vs perennial agriculture, beach, forest, scrub) within the area walked in 2009-2011 and documented (see Data section above for link to coded sheet). 

This dataset has the following limitations:

* teams focused on accessible areas with high visibility so annual agriculture is likely over-represented,
* categories of landuse were assigned on majority bases, so if 60% of field was ploughed and 40% was scrub, the unit is marked as annual agriculture; and
* the data is 10 years younger than the IKONOS image, captured in 2001, which can cause discrepancies. Visual checks and subsetting to areas that look best will be necessary.

Specify your approach and elaborate your solution here:
```{r}
library(raster)
library(sf)
```

Let's load the IKONOS imagery (multiband)
```{r}
# East first
kaze <-brick("data/KazE.tif")
plotRGB(kaze, stretch= "lin")

# West next
kazw <-brick("data/KazW.tif")
plotRGB(kazw, stretch= "lin") # check what it looks like
```

Review the attributes and their correlation
```{r}
# Histograms!
hist(kaze)
hist(kazw)

# Rescale histograms to be consistent
par(mfrow = c(1, 1)) # reset plotting window
hist(kazw, xlim = c(0, 1600), ylim = c(0, 50000), breaks = seq(0, 1500, by = 100))
```
```{r}
pairs(kazw)
pairs(kaze)
```
Now, let's calculate the NDVI using raster algebra or overlay
```{r}
# Extract bands 3 and 4 (Red and Near-Infrared)
kaze3 <- kaze[[3]]
kaze4 <- kaze[[4]]
ndvi <-  overlay(kaze4, kaze3, fun=function(x,y){(x-y)/(x+y)})
plot(ndvi)
hist(ndvi)
minValue(ndvi)
```
```{r}
# Reclassify if subzero values bother you
mat <- cbind(-0.8,0,0)
nvdi_0 <- reclassify(ndvi, rcl = mat)
plot(nvdi_0) # lots of bare soils reflect near 0 (it might be good to try SAVI)
```
# Supervised classification: Random Forest
The Random Forest classification algorithm is an ensemble learning method that is used for both classification and regression. In our case, we will use the method for classification purposes. Here, the Random Forest method takes random subsets from a training dataset and constructs classification trees using each of these subsets. Trees consist of branches and leaves.

Branches represent nodes of the decision trees, which are often thresholds defined for the measured (known) variables in the dataset. Leaves are the class labels assigned at the termini of the trees. Sampling many subsets at random will result in many trees being built. Classes are then assigned based on classes assigned by all of these trees based on a majority rule, as if each class assigned by a decision tree were considered to be a vote.

Skipping Vegetation Continuous Field (VCF) , we can try with bands 2,3,4,ndvi
```{r}
kaze2 <- kaze[[2]]
covs <- brick(kaze2,kaze3,kaze4,ndvi, nvdi_0)
plot(covs)
```
Relabel all the layers, especially the NDVI for later easy interpretation

```{r}
names(covs) <- c("band2", "band3", "band4", "NDVI", "NDVI0")
plot(covs)

```
## Unsupervised Classification with kmeans
Before the random forest, let's check what an unsupervised classification can do, so we can see how much (little) random forest improves on it.
```{r}
kaz_df <- as.data.frame(covs)  
set.seed(99)
cluster_kaz <- kmeans(kaz_df, 5) ### kmeans, with 5 clusters
str(cluster_kaz)

```

Now, convert the dataframe with cluster information into a raster for plotting with raster() and setValues() functions.
```{r}
clusters <- raster(covs)   ## create an empty raster with same extent than kaz
clusters <- setValues(clusters, cluster_kaz$cluster) # convert cluster values into raster
clusters
plot(clusters)
plot(clusters,col=c("dark green", "brown", "blue", "grey","green"))
```
The result is actually pretty good, with water, forest and the bare/urban areas coming clearly out. Water is most differentiated, forest and urban areas a bit mixed, but still outlined. Cropland is a mess, because it combines bare harrowed fields as well as seedlings and young crops and so multiple signatures will need to be fused here.


# Back to Supervised Classification

## Select and Prepare Training Data
Let us do a classification for 2001 using five classes: forest, bare surface, cropland, urban, and water. While cropland is likely to be causing problems due to combination of bare ground and seedlings, and might be best separated into such categories, lets' start simply with water and forest and later we can diversify. A simple classification like this one could be useful, for example, to construct a forest mask for the year 2001.
```{r}
## Load the training polygons as a simple features object
library(sf)
trainingPoly <- st_read("data/Classification/kaz_training_aggr.shp")
## Superimpose training polygons onto NDVI plot
plot(ndvi)
plot(trainingPoly, add = TRUE)
```
Check the labelling of the training polygons
```{r}
# the classification is hiding in Descr_bg field
class(trainingPoly$Descr_bg)

#convert to factor
trainingPoly$Class <- as.numeric(trainingPoly$Descr_bg)
trainingPoly
```

To train the raster data, we need to convert training polygons to the same type, using rasterize(). 
```{r}
## Assign 'Class' values to raster cells (where they overlap)
classes <- rasterize(trainingPoly, ndvi, field='Class')
plot(classes)

## Plotting
# Define a colour scale for the classes (as above)
# corresponding to: cropland, forest, wetland
cols <- c("light blue", "pink", "green","red", "dark green", "light green", "brown", "yellow")

## Plot without a legend
plot(classes, col=cols, legend=FALSE)
## Add a customized legend
legend("topright", legend=c("water", "urban","meadow","bareground","forest", "seedlings","cropland", "beach"), fill=cols, bg="white")
```
The goal in preprocessing these data is to have a table of values representing all layers (covariates) with known values/classes. To do this, we will first need to create a version of our RasterBrick only representing the training pixels. Here, we use the mask() function from the raster package.

```{r}
covmasked <- mask(covs, classes)
plot(covmasked)
```
```{r}
## Combine this new brick with the classes layer to make our input training dataset
names(classes) <- "class"
trainingstack <- addLayer(covmasked, classes)
plot(trainingstack)

```
Now we convert these data to a `data.frame` representing all training data. This `data.frame` will be used as an input into the RandomForest classification function. We will use `getValues()` to extract all of the values from the layers of the RasterBrick.
```{r}
## Extract all values into a matrix
valuetable <- getValues(trainingstack)
valuetable <- na.omit(valuetable)

#Convert the matrix to a data.frame and inspect the first and last 10 rows.
valuetable <- as.data.frame(valuetable)
head(valuetable, n = 10)
tail(valuetable, n = 10)
```
Now that the training dataset is a data.frame, let’s convert the class column into a factor (since the values as integers don’t really have a meaning).


```{r}
valuetable$class <- factor(valuetable$class, levels = c(1:8))
```
Now we have a convenient training data table which contains, for each of the three defined classes,values for all covariates. Let’s visualize the distribution of some of these covariates for each class. To make this easier, we will create 3 different data.frames for each of the classes. This is just for plotting purposes, and we will not use these in the actual classification.

```{r}
val_water <- subset(valuetable, class == 1)
val_urban <- subset(valuetable, class == 2)
val_forest<- subset(valuetable, class == 5)
val_beach <- subset(valuetable, class == 8)

## NDVI
par(mfrow = c(2, 2))
hist(val_water$NDVI, main = "water", xlab = "NDVI", 
     xlim = c(-1, 1), ylim = c(0, 4000), col = "blue")
hist(val_urban$NDVI, main = "urban", xlab = "NDVI", 
     xlim = c(-1, 1), ylim = c(0, 4000), col = "pink")
hist(val_forest$NDVI, main = "forest", xlab = "NDVI", 
     xlim = c(-1, 1), ylim = c(0, 4000), col = "dark green")
hist(val_beach$NDVI, main = "beach", xlab = "NDVI", 
     xlim = c(-1, 1), ylim = c(0, 500), col = "yellow")

par(mfrow = c(1, 1))
```

## Run Random Forest classification
We build the Random Forest model using the training data. For this, we will use the `randomForest` package in R. Using the `randomForest()` function, we will build a model based on a matrix of predictors or covariates (ie. the first 5 columns of valuetable) related to the response (the class column of valuetable).
```{r}
## Construct a random forest model
# Covariates (x) are found in columns 1 to 5 of valuetable
# Training classes (y) are found in the 'class' column of valuetable
## Caution: this step takes fairly long!
# but can be shortened by setting importance=FALSE
# Check for randomForest package and install if missing
if(!"randomForest" %in% rownames(installed.packages())){install.packages("randomForest")}

library(randomForest)
modelRF <- randomForest(x=valuetable[ ,c(1:5)], y=valuetable$class, importance = TRUE)

saveRDS(modelRF,"data/modelRF.rds")
```

The resulting object from the randomForest() function is a specialized object of class randomForest, which is a large list-type object packed full of information about the model output. Elements of this object can be called and inspected like any list object.

```{r}
# Inspect the structure and element names of the resulting model
modelRF
class(modelRF)
str(modelRF)
names(modelRF)
## Inspect the confusion matrix of the OOB error assessment
modelRF$confusion
# to make the confusion matrix more readable
colnames(modelRF$confusion) <- c("cropland", "forest", "wetland", "class.error")
rownames(modelRF$confusion) <- c("cropland", "forest", "wetland")
modelRF$confusion
```
Since we set importance=TRUE, we now also have information on the statistical importance of each of our covariates which we can visualize using the varImpPlot() command

```{r}
varImpPlot(modelRF)
?importance()
```

Apply the model to the rest of the image and assign classes to all pixels. Note that for this step, the names of the raster layers in the input brick (here covs) must correspond exactly to the column names of the training table. We will use the predict() function from the raster package. This function uses a pre-defined model to predict values of raster cells based on other raster layers. This model can be derived by a linear regression, for example. In our case, we will use the model provided by the randomForest() function.

```{r}
predLC <- predict(covs, model=modelRF, na.rm=TRUE)
```
Plot the results:
```{r}
cols <- c("light blue", "pink", "green","red", "dark green", "light green", "brown", "yellow")

## Plot without a legend
plot(predLC, col=cols, legend=FALSE)
## Add a customized legend
legend("topright", legend=c("water", "urban","meadow","bareground","forest", "seedlings","cropland", "beach"), fill=cols, bg="white")
```

```{r}
?writeRaster()
writeRaster(predLC, "data/predLUKaz.tif", format = "GTiff",  overwrite=TRUE)
```

